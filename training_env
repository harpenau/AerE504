import gym
import numpy as np


def make_training_env(env_id):
    '''Create a gym Env from the given env_id and wrap that Env in the
    TrainingEnv class that overrides the reset and step methods by
    altering the data returned from them.'''

    class TrainingEnv(type(gym.make(env_id))):

        def __init__(self, env_id):
            self.env, self.id = gym.make(env_id), env_id
            self.action_space = self.env.action_space
            self.action_shape = self.action_space.shape
            self.observation_space = self.env.observation_space

        @property
        def observation_shape(self):
            '''Try to return the shape of the flattened observation
            array. Otherwise return self.env.observation_space.shape'''
            if hasattr(self.observation_space, 'spaces'):
                return (sum([v.shape[0] for _, v in self.observation_space.spaces.items()]), )
            return self.observation_space.shape

        def concat(self, state):
            '''Try to concatenate the state into one array, otherwise
            return the given state.'''
            try:
                observation, achieved_goal, desired_goal = state.values()
                return np.append(observation, [achieved_goal, desired_goal])
            except AttributeError:
                return state

        def reward(self, state, reward):
            '''Try to return a reward based on the mean squared error
            between desired and achieved goal. Otherwise return reward'''
            try:
                _, achieved, desired = state.values()
                return 1 - (np.square(desired - achieved)).mean(axis=0) + reward
            except AttributeError:
                return reward

        def reset(self):
            '''Return the mutated state vector'''
            return self.concat(self.env.reset())

        def step(self, action):
            '''Change the shape of the action if need-be. Return the
            mutated state, the mutated reward, done, and info'''
            if action.shape != self.action_shape:
                action = action.reshape(self.action_shape)
            state, reward, done, info = self.env.step(action)
            return self.concat(state), self.reward(state, reward), done, info

    return TrainingEnv(env_id)


if __name__ == '__main__':
    env = make_training_env('HandManipulatePen-v0')
    cur_state = env.reset()

    print(env.action_space)
    print(env.observation_space)

    done = False
    while not done:
        env.render()
        state, reward, done, info = env.step(env.action_space.sample())
        print(reward)
#         break
